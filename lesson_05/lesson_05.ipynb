{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урок 5. Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 125)\n",
      "x_test shape: (25000, 125)\n",
      "Построение модели...\n",
      "Процесс обучения...\n",
      "125/125 [==============================] - 231s 2s/step - loss: 0.7291 - accuracy: 0.6230 - val_loss: 0.5917 - val_accuracy: 0.7548\n",
      "125/125 [==============================] - 58s 466ms/step - loss: 0.5917 - accuracy: 0.7548\n",
      "Результат при тестировании: 0.5916826725006104\n",
      "Тестовая точность: 0.7548400163650513\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "maxlen = 125\n",
    "batch_size = 200 # увеличьте значение для ускорения обучения\n",
    "\n",
    "print('Загрузка данных...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'тренировочные последовательности')\n",
    "print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "print('Pad последовательности (примеров в x единицу времени)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Построение модели...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Процесс обучения...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1, # увеличьте при необходимости\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Результат при тестировании:', score)\n",
    "print('Тестовая точность:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default Результат при тестировании: 0.3563539385795593\n",
    "Тестовая точность: 0.8421599864959717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxlen = 250\n",
    "Результат при тестировании: 0.3099709153175354\n",
    "Тестовая точность: 0.8687999844551086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxlen = 125\n",
    "batch_size = 200\n",
    "Результат при тестировании: 0.5916826725006104\n",
    "Тестовая точность: 0.7548400163650513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итого:  \n",
    "наибольшее влияние на результат оказало увеличение параметра maxlen (по сути расширение словаря). что итересно, увеличение количества эпох, как и нейронов мало повлияло на результат, но значительно увеличило время обучения (в итоге отказался от их изменения). также пробовал менять дропаут и оптимайзер, но ощутимых улучшений не получил. еще менял batch_size - увеличение ощутимо сокращает время обучения, хотя и незначительно теряем в качестве. и наоборот, с уменьшением тратим больше времени, но немного выигрываем в качестве. здесь как всегда вопрос приоритетов.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "1241/1241 [==============================] - 67s 54ms/step - loss: 2.4477\n",
      "Генерация из посева: he sun. (if you\n",
      "he sun. (if you said the said the said the said the said the said the said the said the said the said the said the ==================================================\n",
      "Итерация #: 1\n",
      "1241/1241 [==============================] - 73s 59ms/step - loss: 2.0205\n",
      "Генерация из посева: herself falling\n",
      "herself falling the master the master the master the master the master the master the master the master the master ==================================================\n",
      "Итерация #: 2\n",
      "1241/1241 [==============================] - 73s 59ms/step - loss: 1.8379\n",
      "Генерация из посева: ground near the\n",
      "ground near the was the was the was the was the was the was the was the was the was the was the was the was the was==================================================\n",
      "Итерация #: 3\n",
      "1241/1241 [==============================] - 76s 61ms/step - loss: 1.7153\n",
      "Генерация из посева: . *    *    *  \n",
      ". *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *  ==================================================\n",
      "Итерация #: 4\n",
      "1241/1241 [==============================] - 65s 52ms/step - loss: 1.6233\n",
      "Генерация из посева:  joined the pro\n",
      " joined the project gutenberg-tm electronic work the the project gutenberg-tm electronic work the the project guten==================================================\n",
      "Итерация #: 5\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.5502\n",
      "Генерация из посева: lad there was n\n",
      "lad there was not it was to the dormouse the rabbit was the dormouse in the the way the dormouse the rabbit was the==================================================\n",
      "Итерация #: 6\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.4916\n",
      "Генерация из посева: ok itself. then\n",
      "ok itself. then she was so the mouse in the sat of the sat of the sat of the sat of the sat of the sat of the sat o==================================================\n",
      "Итерация #: 7\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.4425\n",
      "Генерация из посева: pt getting enta\n",
      "pt getting entaring the dormouse in the poor a shart the words and the poor a shart the words and the poor a shart ==================================================\n",
      "Итерация #: 8\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.3999\n",
      "Генерация из посева:  and much soone\n",
      " and much sooned the works of the works of the works of the works of the works of the works of the works of the wor==================================================\n",
      "Итерация #: 9\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.3623\n",
      "Генерация из посева: t-hole went str\n",
      "t-hole went string to herself, and the more the thing to the dormouse she was to the dormouse she was to the dormou==================================================\n",
      "Итерация #: 10\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.3300\n",
      "Генерация из посева: ve got to go th\n",
      "ve got to go the pabling the dormouse that she had not a little down and she was a little bead a dont to be the ter==================================================\n",
      "Итерация #: 11\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.3007\n",
      "Генерация из посева: she could, if y\n",
      "she could, if you do not a course of the things and the rabbit was the rabbit was the rabbit was the rabbit was the==================================================\n",
      "Итерация #: 12\n",
      "1241/1241 [==============================] - 64s 51ms/step - loss: 1.2743\n",
      "Генерация из посева:  got into it), \n",
      " got into it), and the sheller of the send to herself, the mock turtle in the electronic works in the seaple that t==================================================\n",
      "Итерация #: 13\n",
      "1241/1241 [==============================] - 64s 51ms/step - loss: 1.2491\n",
      "Генерация из посева: se without my s\n",
      "se without my sitter with the great his spection, and the mock turtle said to the permors of the project gutenberg-==================================================\n",
      "Итерация #: 14\n",
      "1241/1241 [==============================] - 63s 51ms/step - loss: 1.2269\n",
      "Генерация из посева: she went on tal\n",
      "she went on talking a back of the sook of the party round to the mock turtle said alice, and the mock turtle said a==================================================\n",
      "Итерация #: 15\n",
      "1241/1241 [==============================] - 63s 51ms/step - loss: 1.2064\n",
      "Генерация из посева: nd alice could \n",
      "nd alice could not the more tures and she was a little shreen she had not a little shreen she had not a little shre==================================================\n",
      "Итерация #: 16\n",
      "1241/1241 [==============================] - 64s 51ms/step - loss: 1.1871\n",
      "Генерация из посева: , with a deep s\n",
      ", with a deep said to herself, and the more thangs the rabbit was to list the white rabbit was to list the white ra==================================================\n",
      "Итерация #: 17\n",
      "1241/1241 [==============================] - 67s 54ms/step - loss: 1.1684\n",
      "Генерация из посева: --found it advi\n",
      "--found it advines and the mock turtle said to herself, and the mock turtle said to herself, and the mock turtle sa==================================================\n",
      "Итерация #: 18\n",
      "1241/1241 [==============================] - 78s 63ms/step - loss: 1.1505\n",
      "Генерация из посева: off her unfortu\n",
      "off her unfortunes to the ground. i dont know what they were all have you grow the rabbit was and the mock turtle a==================================================\n",
      "Итерация #: 19\n",
      "1241/1241 [==============================] - 78s 63ms/step - loss: 1.1342\n",
      "Генерация из посева: urious, you kno\n",
      "urious, you know. i dont know what the duchess and the other was to the table and had to stort to her edea the look==================================================\n",
      "Итерация #: 20\n",
      "1241/1241 [==============================] - 79s 64ms/step - loss: 1.1183\n",
      "Генерация из посева: g the ink, that\n",
      "g the ink, that it was a little shreaked to see it was a little shreaked to see it was a little shreaked to see it ==================================================\n",
      "Итерация #: 21\n",
      "1241/1241 [==============================] - 80s 65ms/step - loss: 1.1029\n",
      "Генерация из посева:  electronic wor\n",
      " electronic works in the permission of the court. the king said to herself, and the more than said the mouse of the==================================================\n",
      "Итерация #: 22\n",
      "1241/1241 [==============================] - 68s 55ms/step - loss: 1.0893\n",
      "Генерация из посева: e an old woman-\n",
      "e an old woman-- the mock turtle said the hatter, and then all the readon of project gutenberg-tm license there was==================================================\n",
      "Итерация #: 23\n",
      "1241/1241 [==============================] - 65s 52ms/step - loss: 1.0738\n",
      "Генерация из посева: ll she was abou\n",
      "ll she was about it, and the mock turtle in a sont of the confusion in the sea. the dormouse shore the rabbit was g==================================================\n",
      "Итерация #: 24\n",
      "1241/1241 [==============================] - 64s 52ms/step - loss: 1.0606\n",
      "Генерация из посева: ll! fetch it he\n",
      "ll! fetch it her head would be so saying to herself, in a word of your hadint out of the table, and the mouse was a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# построчное чтение из примера с текстом \n",
    "with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 15, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
    "NUM_ITERATIONS = 25 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 3\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итого:  \n",
    "лучшего результата удалось добиться с использованием LSTM, увеличением количества эпох и итераций (при ожидаемо ощутимо возросших затратах времени на обучение) - видим как с каждой итерацией выдаваемые предложения выглядят все более правдоподобными. пробовал также менять активатор, функцию потерь, батч-сайз, но принципиальных отличий не заметил**  \n",
    "- o write the words began to come on it some that alice had been to her ear the door of the treat one of the sal  \n",
    "- today. i think i can do not all the terms of the end of the selted to the caterpillar. the mouse had to see it\n",
    "- fetch it her head would be so saying to herself, in a word of your hadint out of the table, and the mouse was a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
